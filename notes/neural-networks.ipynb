{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Given training set $\\{(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)})\\}$    \n",
    "\n",
    "Set $\\Delta_{i, j} := 0$ for all (l,i,j), hence you end up having a matrix full of zeros.    \n",
    "\n",
    "For training example t = 1 to m:\n",
    "\n",
    "1. Set $a^{(1)} := x^{(t)}$  \n",
    "<br>\n",
    "2. Perform forward propagation to compute $a^{(l)}$ for l=2,3,...,L  \n",
    "<br>\n",
    "3. Using $y^{(t)}$, compute $\\delta^{(L)} = a^{(L)}-y^{(t)}$\n",
    "\n",
    "    Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our \"error values\" for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left.  \n",
    "<br>\n",
    "4. Compute $\\delta^{(L-1)},...,\\delta^{(2)}$ using $\\delta^{(l)}=((\\Theta^{(l)})^{T}\\delta^{(l+1)}\\cdot*a^{(l)}\\cdot*(1-a^{(l)})$\n",
    "\n",
    "    The delta values of layer *l* are calculated by multiplying the delta values in the next layer with the theta matrix of layer *l*. We then element-wise multiply that with a function called *g'*, or g-prime, which is the derivative of the activation function *g* evaluated with the input values given by $z^{(l)}$.\n",
    "\n",
    "    The g-prime derivate terms can also be written out as: $g'(z^{(l)})=a^{(l)}\\cdot*(1-a^{(l)})$  \n",
    "<br>\n",
    "5. $\\Delta_{i,j}^{(l)} := \\Delta_{i,j}^{(l)}+a_{j}^{(l)}\\delta_{i}^{(l+1)}$ or with vectorization, $\\Delta^{(l)} := \\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^{T}$\n",
    "\n",
    "    Hence we update our new $\\Delta$ matrix.\n",
    "\n",
    "    - $D_{i,j}^{l} := \\frac{1}{m}(\\Delta_{i,j}^{(l)}+\\lambda\\Theta_{i,j}^{l})$, if $j\\neq0$.\n",
    "    - $D_{i,j}^{l} := \\frac{1}{m}\\Delta_{i,j}^{(l)}$, if $j=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "\n",
    "To make sure that the backpropagation algorithm is correct, compare the ``gradApprox`` to the computed gradient vector.\n",
    "\n",
    "$$gradApprox = \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization (Symmetry Breaking)\n",
    "\n",
    "Initialize each $\\Theta_{i, j}^{(l)}$ to a random value in $[-\\epsilon, \\epsilon]$ (not related to gradient checking $\\epsilon$). That is, $-\\epsilon\\leq\\Theta_{i, j}^{(l)}\\leq\\epsilon$.\n",
    "\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the \n",
    "number of units in the network. A good choice of $\\epsilon_{init}$ is:\n",
    "\n",
    "$$\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$$\n",
    "\n",
    "where $L_{in} = s_{l}$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning a neural network\n",
    "\n",
    "1. Randomly initialize weights.\n",
    "2. Implement forward propagation to get $h_{\\Theta}(x^{(i)})$ for any $x^{(i)}$.\n",
    "3. Implement code to compute cost function $J(\\Theta)$.\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\partial}{\\partial\\Theta_{j,k}^{(l)}}J(\\Theta)$.\n",
    "5. Use gradient checking to compare $\\frac{\\partial}{\\partial\\Theta_{j,k}^{(l)}}J(\\Theta)$ computed using backpropagation vs using numerical estimate of gradient of $J(\\Theta)$. Then disable gradient checking code.\n",
    "6. Use gradient descent or other advanced optimization methods with backpropagation to try to minimize $J(\\Theta)$ as a function of parameters $\\Theta$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
